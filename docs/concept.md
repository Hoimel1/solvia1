Workflow for MD Simulation-Based Toxicity Prediction of AMPs on Protein-Free RBC Membrane Models (Updated July 2025)

This project aims to predict the hemolytic toxicity of antimicrobial peptides (AMPs) using coarse-grained molecular dynamics (MD) simulations and machine learning. Toxicity is quantified by the experimentally determined HC50 values (the peptide concentration causing 50% hemolysis) for a dataset of ~1500 peptides. The workflow employs the Martini 3 coarse-grained force field and incorporates recent advances in reproducible MD simulation, trajectory-based feature engineering, and interpretable machine learning. A key aspect of our approach is using a protein-free red blood cell (RBC) membrane model – i.e. the membrane is composed of lipids and cholesterol without any embedded membrane proteins – to focus on peptide–lipid interactions. Below, we outline the end-to-end pipeline, detailing each step and justifying modifications that ensure the model is biologically realistic and computationally robust.

⸻

1. Structure Prediction (Containerized AlphaFold 3 Ensemble)

For each AMP sequence, we generate a three-dimensional structure using AlphaFold 3 – a state-of-the-art diffusion-based architecture for protein and peptide folding (the top performer in CASP16 for protein complexes and peptide targets in 2024, with excellent GDT_TS and DockQ scores). To ensure reproducibility and easy deployment across different systems (workstations, cloud nodes, HPC clusters), AlphaFold 3 runs inside a version-pinned container (Docker/Apptainer) that encapsulates all dependencies.

Containerized Setup: We build a multi-stage Docker image (~5 GB) derived from the official AlphaFold 3 repository. Stages include the OS base, CUDA drivers, machine learning libraries (PyTorch & JAX), and the AlphaFold 3 code. The final image is tagged with a specific commit hash (alphafold3:<git-sha>), making runs exactly reproducible. GPU support is enabled via NVIDIA’s Container Toolkit, and on clusters without Docker we use Apptainer (Singularity) to run the same image. A helper script (docker2singularity.sh) automates conversion of the Docker image to an Apptainer SIF, which is useful on HPC systems where Docker is unavailable.

Reference Data: Large databases (sequence and structure templates) needed by AlphaFold – such as UniRef, BFD, PDB70, and AlphaFold’s parameters – are provided via read-only mount inside the container. To reduce overhead for small peptides, we offer a “lite” database bundle (e.g., UniRef50 and PDB70 only, <60 GB) selectable by a configuration flag. This accelerates searches for short sequences while still providing sufficient coverage.

Batching Predictions: To maximize GPU utilization, we batch multiple peptide sequences in one AlphaFold run when possible. Up to 32 short peptides (e.g. 10–30 amino acids each) can be processed in parallel on a single 16GB GPU, depending on memory. This batched approach significantly reduces per-sequence runtime by sharing the overhead of loading the model and databases.

Multiple Models & Quality Filtering: We generate five independent structure predictions per peptide, each with a different random seed for the neural network initialization. These five models are then structurally clustered (using Cα RMSD) to assess diversity, and the centroid of the largest cluster is chosen as the representative structure for that peptide. We apply strict quality criteria: any model with a mean pLDDT < 80 or a high MolProbity clash score (>20) is discarded, and if the top models fail these thresholds, the peptide is flagged for manual inspection. In practice, for most AMPs (which are often helical), AlphaFold produces high-confidence structures. We further run a quick energy minimization (AmberTools reduce and minimization in vacuum) on the chosen model to resolve any residual steric clashes. If a peptide is predicted to be mostly disordered (low-confidence structure), we still proceed with coarse-graining (see Section 2), but the simulation will treat it as flexible.

Ensemble Predictions: While AlphaFold 3 is our primary tool, we recognize that no single predictor is infallible. Thus, for peptides where AlphaFold’s confidence is borderline (e.g., pLDDT around 70-80), we optionally employ an alternative structure prediction method (such as an updated RoseTTAFold or a simple helical peptide builder) to generate an ensemble of starting conformations. This cross-validation ensures that a problematic AlphaFold output (e.g., an incorrectly folded or overly compact structure) doesn’t propagate through the pipeline. In cases of low-confidence predictions, we might run a short equilibration MD in solution to let the peptide adopt a more natural conformation before proceeding to membrane simulations.

Output and Reproducibility: The predicted structure (usually named ranked_1.pdb from AlphaFold) along with metadata (pLDDT per residue, seed used, container version hash) is saved under data/interim/{sample_id}/. We record the random seed and container digest for each run, ensuring that the prediction is exactly reproducible later if needed. The Snakemake workflow’s rule predict_structure attaches the container and requests appropriate resources (memory, runtime, GPUs) for each batch. A continuous integration (CI) test on GitHub (using a small peptide example and CPU-only mode) is set up to verify that the container and script function correctly after any pipeline updates. This guarantees that future changes (e.g., library upgrades) do not break the structure prediction step.

By leveraging containerized AlphaFold 3 and (when necessary) ensemble modeling, we establish a consistent, reproducible, and high-quality starting point for all peptides. Each peptide’s initial structure is obtained under identical conditions, eliminating software environment variability. These standardized peptide models will be used for coarse-graining in the next step, providing a uniform foundation for subsequent membrane simulations.

⸻

2. Coarse-Graining of Peptides (martinize2 with Martini 3 & IDP Options)

Every high-confidence peptide structure from Step 1 is converted into a coarse-grained (CG) representation using the Martini 3 force field via martinize2. We perform this conversion deterministically and apply quality controls to ensure a stable CG model. The workflow avoids any biasing potentials (no Gō-like structure biases are used) unless needed for maintaining secondary structure. We do, however, allow the use of Martini’s ElNeDyn elastic network in an adaptive manner for structured peptides, and importantly, we incorporate the new Martini3-IDP model for peptides that are disordered, to prevent over-constraining flexible sequences.

2.1 Deterministic martinize2 Execution

We use martinize2 v0.14.0 (with Vermouth 0.14.x library) packaged in a lightweight container (~400 MB, tagged martinize2:v0.14.0-<git-sha>). The exact martinize2 commit and container SHA are stored in a cg_metadata.db for traceability. The Snakemake rule coarse_grain specifies this container and runs martinize2 with fixed random seeds for deterministic output.

Command Template: The typical command executed is:

martinize2 -f {input_model}.pdb -o {peptide}.top -x {peptide}_cg.pdb \
          -ff martini3001 -water martini3 -dssp mkdssp \
          --seed 42 --pdb-segid {elastic_opts}

Here, martini3001 refers to Martini 3.0.0 with the updated amino acid parameters. We include side chain fixes by default (martinize2 v0.14 does this automatically, so no -noscfix flag is needed). The elastic_opts are added only if an elastic network is to be applied (see below). We set a fixed random seed (42) so that martinize2’s internal random decisions (like bead naming or elastic bond selection) are consistent across runs.

Adaptive Elastic Network: For peptides that are largely helical or structured, we enable an adaptive ElNeDyn elastic network. In Snakemake’s config, a flag elastic_mode can be set to "adaptive" or "none". If adaptive, martinize2 initially connects all backbone beads with elastic bonds (500 kJ/mol/nm² force constant, elastic bond length 0.5 nm, upper cutoff 0.9 nm). Then, a custom script prune_elastic.py removes any elastic bonds involving beads from regions with low AlphaFold confidence (pLDDT < 70). The idea is to stabilize well-formed helices or β-strands with a mild network, while leaving terminal or disordered regions free. This prevents unstructured tails from being artificially restrained, preserving the AMP’s natural flexibility – important for mechanisms like carpet models where peptides may lie flat on the membrane. If elastic_mode is "none", we skip adding any elastic bonds, letting the peptide move completely freely (this is used in comparative tests to see the effect of the network).

Handling Disulfides: Martinize2 automatically detects disulfide bonds between cysteine residues and includes them in the topology (unless told otherwise). We rely on this to handle any cystine-bridged peptides correctly (though most of our AMPs are linear without disulfides).

Martini3-IDP for Disordered Peptides: Notably, we have integrated the Martini3-IDP parameters (released in 2025) for peptides predicted to be intrinsically disordered ￼. Martini3-IDP is a modified set of protein parameters that yields more expanded conformations (reducing the compaction bias of standard Martini 3 for disordered sequences). In practice, if a peptide’s AlphaFold model had low confidence (indicating a random coil) or if the sequence is known to lack stable secondary structure, we toggle a flag that instructs martinize2 to use Martini3-IDP bead definitions. Under the hood, this applies refined Lennard-Jones and bonded parameters calibrated for IDPs (fully implemented in martinize2 v0.14 as per the Martini team’s update). Using Martini3-IDP ensures that such peptides sample realistic conformations in simulation – they won’t artificially collapse into a tight globule but remain dynamic and extended, matching experimental radii of gyration for IDPs ￼. This is crucial for our toxicity predictions because some AMPs function in a disordered state; misrepresenting them as overly compact could alter their membrane interaction behavior.

Output Artifacts: The coarse-graining step produces: (1) a coarse-grained PDB (*_cg.pdb) with the bead representation of the peptide, (2) a topology file (*.top and an included *.itp file) describing the peptide in Martini 3 terms, (3) if elastic network is used, a log of elastic bonds and a pruned elastic network include file, and (4) a checksum file of the input to ensure traceability. We embed the AlphaFold model seed in filenames (e.g., peptideX_s3_cg.pdb for the model based on AF3 seed 3) for clarity. All CG outputs are stored under data/cg/{sample_id}/. A Snakemake checkpoint ensures that only peptides passing the quality filters in Step 1 are processed.

2.2 Quality Control (CG Model QC)

We implement several QA checks on the coarse-grained models to prevent any problematic peptide topology from proceeding to expensive simulations:
	•	Energy Minimization in Vacuum: We run a brief steepest-descent minimization (100 steps) on the isolated CG peptide (with an appropriate Martini 3 vacuum force field) to ensure the topology has no pathological energies. The maximum force after 100 steps must be <1000 kJ/mol/nm; if not, it indicates something like overlapping beads or an incorrect bond, and the peptide is flagged. Typically, martinize2 outputs are already reasonable, so this check nearly always passes.
	•	Backmapping RMSD: We perform a round-trip conversion as a sanity test. Using the CG structure, we apply the backward mapping tool (Backward 2025.2) to reconstruct an atomistic structure, then align that to the original AlphaFold model and compute the Cα RMSD. If the RMSD > 0.45 nm, we consider that the CG representation might be too dissimilar from the input structure (perhaps due to an aggressive elastic network or errors in mapping). RMSD < 0.3 nm is expected for well-behaved cases. A few tenths of nm difference is normal because the CG model doesn’t have sidechains, etc., but anything large suggests a problem (the peptide might have mirrored or rotated incorrectly). Such cases are examined manually.
	•	Elastic Network Statistics: If an elastic network was applied, we record the number of elastic bonds and their average length. We compare these to typical values for peptides of similar length. Any outlier (e.g., a very large number of elastic bonds or very long elastic bonds) could mean our adaptive pruning failed or the peptide has an odd topology. We include these stats in an elastic_qc.tsv for reference. In practice, the adaptive network yields elastic bonds mostly within secondary structure elements and very few (or none) in unstructured regions, which is the desired outcome.
	•	CI Regression Test: As part of our CI pipeline, we include a regression test for coarse-graining. Two small peptides (20 residues each, one mostly helical, one random coil) are coarse-grained in GitHub Actions (CPU-only) and their outputs hashed. We ensure that the SHA-256 of the generated .top files remains consistent with a known good output. This alerts us if an update to martinize2 or a container change alters the CG results unexpectedly.
	•	Dashboard Reporting: We aggregate all QC metrics into a SQLite database (cg_qc.db) and generate an HTML report (using Plotly) summarizing them for all peptides. This report (produced with snakemake --report cg_qc.html) visualizes distributions of CG model properties (energy, RMSD, elastic bonds count, etc.) and highlights any outliers for review. So far, no peptide in our dataset has failed the CG stage; this is partly thanks to the rigorous filtering in structure prediction and the maturity of the Martini 3 force field for peptides.

2.3 Scientific Rationale and Best Practices

Our coarse-graining approach is designed with both scientific accuracy and reproducibility in mind:
	•	Relevance to AMPs: Many AMPs are helices or have defined structures when bound to membranes. The optional adaptive elastic network helps preserve these secondary structures during simulation (preventing an α-helix from unrealistically uncoiling, for instance) without freezing the peptide. By masking low-confidence regions, we ensure that flexible termini or loops remain flexible. This approach is in line with recommendations from the Martini workshop 2025, where secondary-structure-aware elastic networks were suggested as a way to improve CG protein dynamics without introducing bias ￼ ￼. The end result is that our CG peptides maintain their intended fold (important for peptides that require a helix to insert into membranes) yet can sample necessary conformational changes.
	•	Use of Martini3-IDP: Incorporating Martini3-IDP parameters for disordered peptides is cutting-edge practice. Standard Martini 3 sometimes over-stabilized compact conformations for IDPs, so the improved model addresses that by tuning protein–protein and protein–water interactions ￼. By adopting Martini3-IDP for relevant peptides, we ensure our simulations don’t artificially suppress the dynamics of peptides that should be unstructured. This yields more realistic peptide behavior in simulations, which is crucial for the subsequent analysis of toxicity (since a disordered peptide might carpet the membrane surface differently than a rigid helix would).
	•	Excellence and Precision: We use the latest stable martinize2 release and carefully document all parameters. The integration of DSSP (to read secondary structure from the AlphaFold model) means our bead mapping accounts for helical regions (affecting how backbone beads are connected and whether an elastic network is applied). All steps are fully deterministic – fixed seeds, containerized software – ensuring any researcher can regenerate the exact CG structure from the same input. This precision is necessary for a high-throughput study: it eliminates run-to-run variability, so differences in outcomes for different peptides truly stem from peptide properties, not random quirks of the model building.
	•	Reproducibility: By pinning container versions and hashing outputs, we guarantee that the CG model of, say, melittin, is the same whether generated today or a year from now. The entire environment (martinize2 version, Python libraries, etc.) is encapsulated, which is a pillar of reproducible computational science. This also aids collaboration – if someone else wants to simulate the same peptide, they can use our container and get identical topologies, making results directly comparable.
	•	Downstream Compatibility: We designed the outputs to plug directly into the next steps (membrane building and MD setup). The data/cg/{id}/ directory structure and file naming (with consistent use of the peptide’s ID) allow Snakemake to automatically pick up the right files for building combined systems. We’ve kept the format aligned with Martini’s INSANE tool requirements (e.g., the presence of a #ifdef POSRES in the topology if position restraints are needed, etc.). Because we adhered to the usual Martini conventions, we didn’t need to modify the subsequent INSANE or GROMACS pipeline when we updated our martinize2 version or elastic network usage – everything remains compatible.

In summary, our coarse-graining step produces validated, ready-to-simulate CG peptide models for each AMP. The use of adaptive elastic networks (when appropriate) and Martini3-IDP for disordered cases sets this pipeline apart as it tailors the model to the peptide’s nature, rather than one-size-fits-all. This attention to detail at the CG stage lays a strong foundation for physically meaningful membrane simulation outcomes.

⸻

3. Membrane Construction (INSANE Tool with Updated RBC-Like Lipid Composition)

With coarse-grained peptide models in hand, we next build a lipid bilayer system mimicking a human red blood cell membrane, around each peptide. We use the INSANE (INSert membrANE) tool to assemble a bilayer with the desired lipid composition and asymmetry. Our membrane model is protein-free – meaning we do not include any membrane proteins in the bilayer – focusing only on lipid constituents and cholesterol. This simplification isolates the interactions of AMPs with the lipid membrane, which is the primary driver of hemolysis, while keeping system size manageable. Notably, real RBC membranes are about 50% protein by mass (e.g., band 3, glycophorin, and spectrin network) ￼, but including those in simulations would add enormous complexity and variability. By using a pure lipid/cholesterol membrane, we emulate the RBC lipid environment without the confounding effects of membrane proteins.

Target Lipid Composition: We choose a representative composition based on experimental data for human erythrocyte plasma membranes. The lipid mixture (by mole percent) is roughly: 35–40% phosphatidylcholine (PC, modeled as POPC plus a small fraction of sphingomyelin analog), 25–30% phosphatidylethanolamine (PE, modeled as POPE), ~10% phosphatidylserine (PS, POPS), and the remainder (20–30%) cholesterol (cholesterol molecules in Martini). We also include a minor component of phosphatidylcholine with sphingomyelin-like properties (Martini uses POPC to represent both PC and SM class lipids, since SM has the same headgroup as PC). This simplified composition captures the key features: PC and PE are the most abundant phospholipids in RBCs (around 40% and 30% of total phospholipids, respectively), and PS is a smaller fraction confined to the inner leaflet ￼. Crucially, cholesterol is extremely abundant in RBC membranes – nearly equimolar to total phospholipids, i.e. ~40–50% of the lipid molecules ￼. We set our cholesterol content to ~45% of total lipids (significantly higher than earlier versions of our model which used 20–30%) to reflect this. The high cholesterol level is expected to increase the membrane’s thickness and order (cholesterol’s condensing effect on lipid tails) and reduce permeability, aligning the model closer to real RBC membrane properties ￼.

Leaflet Asymmetry: RBC membranes are highly asymmetric: the outer (exoplasmic) leaflet consists mostly of choline lipids (PC and sphingomyelin) and a large share of cholesterol, whereas the inner (cytoplasmic) leaflet is enriched in amino-phospholipids (PE and almost all PS) ￼. Additionally, new lipidomics research indicates the inner leaflet actually contains >50% more phospholipid molecules than the outer leaflet ￼. In other words, the bilayer has a lipid number imbalance (the inner leaflet is more crowded with lipids), which is thought to be enabled by cholesterol preferentially partitioning to the outer leaflet to offset the area difference ￼ ￼. To incorporate asymmetry, we configure INSANE with different lipid ratios for each leaflet: the upper (outer) leaflet is set to be ~80% PC (to also represent SM) plus ~20% cholesterol, and no anionic lipids; the lower (inner) leaflet is ~50% PE, ~20% PS, and ~30% cholesterol. This ensures essentially all PS resides in the inner leaflet (matching biology, as external PS is a signal for cell removal), and the majority of PE is inner as well. The outer leaflet ends up nearly all PC/SM + cholesterol, which is in line with experimental measurements (classic studies showed >85% of outer-leaflet phospholipids are PC/SM ￼). We also attempt to mimic the lipid count imbalance: when building the system, we instruct INSANE to place slightly more lipid molecules in the inner leaflet than the outer (roughly a 3:2 ratio) to approximate the >50% phospholipid overabundance inside ￼. In practice for a planar patch, this is tricky (a significantly higher density of lipids in one leaflet can cause curvature or stress), so we implement it modestly – our inner leaflet might have ~10–20% more molecules than the outer. This asymmetry in number, combined with differential cholesterol distribution (we put somewhat more cholesterol in the outer leaflet by proportion), yields a stable but asymmetric bilayer. It captures the essential lateral packing asymmetry of RBC membranes: a loosely packed inner leaflet and a tighter outer leaflet with cholesterol. We note that our default simulations use a planar patch (semi-periodic boundary conditions); thus, the leaflet imbalance is a trade-off between realism and stability. We monitor the membrane for any spontaneous curvature. (Alternatively, INSANE can build a small closed vesicle to naturally accommodate different inner/outer lipid counts – see optional extensions below.)

Implementation with INSANE: The membrane building is done within the Martini 3 environment (same container as for coarse-graining, which includes Python2 and the INSANE script). For each peptide, we run INSANE via a command like:

python2 insane.py -o system.gro -p system.top -box 10,10,${Z} \
  -sol W -salt 0.15 -chargeneutral \
  -u "POPC:0.4 CHOL:0.4" -l "POPE:0.25 POPS:0.1 CHOL:0.25" -asym

Here -box 10,10,${Z} defines the box in x,y (10×10 nm² base) and a z dimension large enough to fit the bilayer plus water (we set Z ≈ 12–15 nm for ~5 nm bilayer + sufficient bulk water on each side). The -sol W -salt 0.15 adds Martini water and ~150 mM NaCl (with -chargeneutral ensuring net neutralization if the peptide or PS lipids carry charge). The -u and -l flags specify the upper and lower leaflet compositions. In this example, the outer leaflet is 40% POPC, 40% cholesterol (and the remaining 20% can implicitly be other lipids like we sometimes include a small fraction of glycolipids or sphingomyelin analog, but those are omitted here for simplicity). The inner leaflet is set to 25% POPE, 10% POPS, 25% cholesterol (and the balance ~40% would be other lipids, but since we want mainly PE/PS, we might actually adjust to something like 30% PE, 10% PS, 0% PC, 30% cholesterol, with 30% unspecified – INSANE would then fill unspecified with default water or balanced lipid? In practice, we explicitly list all components to avoid ambiguity). The -asym flag is critical – it tells INSANE to treat the -u and -l compositions separately for the two monolayers, rather than averaging them. INSANE then randomly packs lipids in a two-leaflet configuration with the given proportions. It also embeds the peptide into the system, if an input coordinate file (the CG peptide) is provided with the -f option (not shown above). We position the peptide initially ~1 nm above the upper leaflet (or below the lower leaflet, depending on the peptide’s presumed orientation). If a peptide is cationic (as most AMPs are), it will likely interact with the anionic PS-rich inner leaflet; however, since our default placement is above the outer leaflet, effectively we simulate the scenario of a peptide approaching from outside the cell (which is reasonable for hemolysis assays). We can invert the orientation for peptides known to bind inner leaflet (if they somehow cross the membrane) but that’s not typical in experiments.

System Size: Each assembled membrane patch is about 10×10 nm², containing on the order of 5000 lipid molecules (given our high cholesterol content and asymmetry, the exact count might be ~2200 lipids outer, 2800 lipids inner, plus ~2000 cholesterol distributed, totaling ~5000). Including water and ions, each system has ~50,000 CG particles. This size is large enough to accommodate a peptide (or even a peptide dimer, if one formed) and capture local membrane perturbations, but small enough to simulate for microseconds reasonably. We also consider building replicas of each system: since INSANE placement is stochastic, we often create 2 independent membrane configurations for the same peptide (with different random seeds in INSANE) to sample different initial lipid arrangements. These serve as replicate simulations to ensure our results aren’t an artifact of a particular initial lipid distribution.

Energy Minimization & Equilibration: After INSANE assembles the system (peptide + bilayer + water/ions), we run a quick energy minimization using GROMACS (steepest descent, typically converging in <1000 steps) to remove any bad contacts. Then, we equilibrate the membrane system in two phases: (1) 100 ps NVT at 310 K (Berendsen or V-rescale thermostat, with position restraints on the peptide heavy beads to keep it from crashing into the membrane initially), and (2) 5–10 ns of NPT at 310 K, 1 bar (semi-isotropic coupling with Berendsen barostat for the first 1 ns to gently adjust, then Parrinello-Rahman for remaining time). During NPT, we release peptide restraints after ~1 ns, allowing it to start interacting with the membrane. By the end of equilibration, the area per lipid stabilizes and the peptide has typically either adsorbed onto the membrane surface or begun inserting if it is very hydrophobic. We save the equilibrated configuration (system_equil.gro) to serve as the starting point for production MD.

Quality Checks on Membrane Systems: We verify several properties at this stage:
	•	The bilayer thickness (distance between phosphate bead layers) is around 4.5–5 nm, consistent with a cholesterol-rich membrane ￼. If our bilayer is significantly thinner (<4 nm), it could mean lipid depletion or a packing issue; significantly thicker (>5.5 nm) could indicate too high cholesterol or an unrealistic packing – we would then adjust composition or rebuild.
	•	The average area per lipid in the outer vs. inner leaflet: We compute this from the box dimensions and lipid counts. Outer leaflet PC/SM typically have an area ~0.60 nm² in fluid phase without cholesterol, but with 40% cholesterol it should drop to ~0.50 nm² (cholesterol condenses the lipids) ￼. The inner leaflet, rich in PE/PS (unsaturated tails) and slightly less cholesterol, might have area ~0.55–0.65 nm². We expect an asymmetry: inner leaflet area per lipid slightly larger, which ties to the notion of inner leaflet having more lipids but being loosely packed ￼. We ensure these values are reasonable; if not, it may signal an imbalance. In initial tests, our area per lipid came out close to literature values for similar compositions, giving confidence in the model.
	•	No peptide or lipid overlaps: We check that the peptide is not starting inside the hydrophobic core (unless intentionally placed there). gmx mindist confirms the minimum peptide-lipid distance is >0 nm initially (no illegal overlaps). Any severe steric clashes would show up during minimization as huge forces, which we did not observe after careful placement.
	•	Preservation of Asymmetry: We count lipids of each type per leaflet from the equilibrated structure to ensure none have flipped sides spontaneously. As expected, in a ~10 ns coarse-grained equilibration, essentially zero flip-flop of PC, PE, or PS is observed (those events are kinetically very slow without scramblases). Cholesterol, which can flip-flop rapidly, might redistribute somewhat between leaflets – we watch for that. If we started with, say, 60% of cholesterol in the outer leaflet (to emulate outer cholesterol enrichment), it could even out to some degree. We don’t enforce a freeze on cholesterol because its free diffusion is part of the model’s physical realism ￼. Typically, the system reaches a quasi-steady cholesterol distribution that still favors the outer leaflet by a bit. The key is that virtually all PS remains in the inner leaflet, as it should (if any PS appeared in outer, that would indicate an anomaly).

Optional Extensions: The pipeline is modular, and we can adjust the membrane model for specific investigations:
	•	Membrane Proteins: By default, our membranes are protein-free to isolate peptide-lipid interactions. However, if one wanted to study peptide–protein interactions (e.g., how an AMP might bind to a membrane protein or how proteins affect peptide access), we could insert a coarse-grained membrane protein. For example, we have a topology for Glycophorin A (a single-pass RBC membrane protein) that we could include. We’d use INSANE’s ability to insert multiple solutes: including the peptide and the protein in the -f input. This increases system size and complexity, so we reserve it for follow-up studies. None of our production toxicity predictions include membrane proteins – effectively, we’re simulating something akin to an RBC lipid vesicle (ghost) rather than a full cell membrane with proteins. This is justified because experiments like hemolysis assays often use RBC ghosts or focus on membrane disruption which is largely a lipid-driven phenomenon. Indeed, peptides causing hemolysis generally do so by creating pores in the lipid bilayer; the presence or absence of specific membrane proteins is secondary in many cases.
	•	Curved Membranes (Vesicles): We can also build closed vesicles using INSANE’s -vesicle flag, to study curvature effects or to accommodate large leaflet number differences naturally. A vesicle allows the inner leaflet to physically contain more lipids than the outer (since the inner surface area is smaller for a given vesicle), exactly as in a real cell. In a small vesicle (e.g. 20 nm diameter), the inner leaflet has significantly less area, which is the opposite of RBC’s case (RBC is a biconcave disc ~8 µm across, not a tiny vesicle). However, we could approximate an RBC ghost by a large vesicle ~100 nm in diameter, where the curvature is minor but allows inner leaflet packing. This is computationally expensive (number of lipids would be huge), so for now we stick to flat patches. But the pipeline supports a toggle (membrane_type: vesicle in config) that would invoke building a vesicle of specified size instead of a planar patch, if needed for specialized experiments.
	•	Different Cholesterol Levels: We might vary cholesterol content to see how membrane rigidity influences peptide action. For instance, one could create a low-cholesterol variant (10%) and a high-cholesterol variant (50%) and compare peptide insertion depths or induced leakage. Since our pipeline is high-throughput, we could screen a subset of peptides on multiple membrane compositions to detect any correlation between cholesterol and toxicity (some AMPs are known to be less effective on cholesterol-rich membranes ￼). Our workflow would simply take an array of compositions and produce parallel simulations for each.
	•	Other Lipid Species: The current model lumps all PC into POPC and all SM into POPC as well (since Martini doesn’t have a separate SM bead type, one typically uses POPC with slightly altered tail parameters to mimic SM’s saturated tails). We could explicitly add sphingomyelin or glycolipids (Martini does have some glycolipid models) if we want to explore their effect. RBC outer leaflets also contain cholesterol-rich domains (rafts) which often involve sphingomyelin. We’ve kept the model simple by treating it uniformly, but one extension is to induce a phase-separated domain (perhaps by including a small fraction of dipalmitoyl-PC to see if a gel phase forms). However, given that our interest is in general hemolytic activity, a homogeneous fluid membrane is a reasonable baseline.

Summary: By constructing a realistic, asymmetric, cholesterol-rich, protein-free membrane around each peptide, we set the stage for meaningful simulations of peptide–membrane interactions. The composition and asymmetry are grounded in experimental measurements of RBC membranes ￼ ￼, ensuring that the physical properties (thickness, fluidity, charge distribution) are as close as possible to a red cell membrane. The absence of membrane proteins in the model is a deliberate choice to make the simulations tractable and to focus on the lipid-mediated toxicity mechanism. Any influences of proteins (e.g., anchoring of the spectrin network, or specific peptide-protein binding) are left for future work. The current model essentially represents an erythrocyte “ghost” – a membrane with correct lipid makeup but no cytoskeletal support – which is actually the relevant entity in many hemolysis experiments (where RBCs can be considered as membranes that swell and burst). With equilibrated peptide–bilayer systems ready, we proceed to production MD simulations.

Sources for Section 3:
	1.	Wassenaar et al., JCTC 2015 – Introduction of the INSANE tool for membrane building ￼.
	2.	Himbert et al., Adv. Colloid Interface Sci. 2021 – Lipid composition of RBC membranes (PC ~40%, PE ~30%, PS ~10%) and bilayer thickness ~5 nm in RBC ghosts ￼ ￼.
	3.	Doktorova et al., Cell 2025 – Discovery of >50% phospholipid excess in inner RBC leaflet and role of cholesterol asymmetry ￼.
	4.	Scientific Reports 2020 (Lorent et al.) – ~40% cholesterol in RBC membrane and discussion of its asymmetric distribution between leaflets ￼ ￼.
	5.	Erythrocyte membrane overview (Sci. Direct) – Roughly 50% protein, 40% lipid, 10% carbohydrate by mass in RBC membrane; outer leaflet mainly PC/SM, inner leaflet PE/PS ￼ ￼.

⸻

4. Production MD Simulations (GROMACS with Martini 3, 0.5–1 µs runs)

We conduct coarse-grained MD simulations for each peptide–membrane system to observe the interactions and perturbations that relate to hemolytic activity. Simulations are run using GROMACS 2024 patched for Martini 3, with GPU acceleration. The conditions replicate physiological environment: 310 K temperature, 1 bar pressure (semi-isotropic coupling to allow the membrane area to fluctuate), 150 mM NaCl, and neutral pH (implicitly by neutral zwitterionic lipids and presence of both counter-ions). For each peptide, we run multiple independent simulations (typically 2 replicates, sometimes 3) starting from different initial velocity distributions or different initial lipid packing, to ensure statistical reliability. Each simulation is run for 500 ns, and if needed extended to 1 µs, with a time step of 20 fs. This trajectory length (hundreds of ns) is sufficient in Martini time to capture key events like peptide binding, insertion, membrane thinning, or pore formation, especially since dynamics are faster in CG (Martini roughly maps to 4× speed-up in some processes).

Simulation Parameters (MDP settings):
	•	Integrator & Time Step: We use the leap-frog integrator (integrator = md) with dt = 0.02 ps (20 fs). Martini 3 allows time steps up to ~20–30 fs safely ￼ due to the softer potentials and absence of high-frequency vibrations; 20 fs is a conservative choice that provides good energy conservation ￼. We couple a neighbor-list update every 20 steps (i.e., every 0.4 ps) and a Verlet cutoff scheme with a buffer tolerance of 0.005, letting GROMACS determine an optimal neighbor list radius (usually ~1.2 nm for a 1.1 nm cutoff) to keep pair forces accurate.
	•	Nonbonded Interactions: We set a cutoff of 1.1 nm for both Lennard-Jones and short-range Coulomb interactions (standard for Martini 3). Beyond 1.1 nm, interactions are switched to zero smoothly (potential-shift Verlet scheme) ￼. For Coulomb, we use the reaction-field method with epsilon_r = 15 and no PME by default (Martini’s recommended approach, which effectively mimics a medium of dielectric 15 and has negligible long-range electrostatics beyond the cutoff) ￼. However, we also tested Particle Mesh Ewald (PME) with appropriate settings and found no significant difference in structural outcomes for these neutral-charged systems (peptides are cationic but the counterions and zwitterionic lipids screen them). We stick with reaction-field for performance and consistency with Martini parametrization.
	•	Thermostat: A velocity-rescale thermostat (τ_T = 1.0 ps) maintains temperature at 310 K ￼. We use separate coupling groups for the membrane+peptide and for the solvent (water+ions) ￼. This avoids the “hot solvent, cold membrane” artifact; each group is thermalized independently, which is recommended because the heat capacities differ (the membrane can drift to a different temperature if coupled together with the large solvent bath). With this, we observed stable and correct temperature profiles (both groups hover at 310 K with minimal drift).
	•	Barostat: We employ a Parrinello–Rahman barostat (semi-isotropic) to maintain 1 bar pressure ￼. Semi-isotropic means the x-y plane of the membrane is coupled together and separately from the z-axis (normal to the membrane). We set compressibility to 3e-4 bar^-1 in x-y and z (Martini water is slightly compressible) and τ_P = 12 ps – a relatively long coupling constant to prevent oscillatory instabilities ￼. The longer τ_P smooths pressure adjustments, which is helpful given the small box and the need to maintain asymmetric leaflets without shock. This results in the membrane finding its tensionless state (surface tension ~0) naturally, as indicated by stable area per lipid. Over the first few nanoseconds, the barostat may adjust the box a bit (~1-2% area change) until equilibrium is reached; after that the area fluctuations are small around a mean.
	•	Constraints: All bond lengths involving hydrogen (in polarizable Martini water, if using it) are constrained with LINCS. For Martini 3 proteins, most bonds are not hard constraints (they’re flexible or constrained by virtual sites), so the main constraints are in water (which uses the standard MARTINI flexible water model, so actually no constraints, just angles). We run with LINCS high order (order 8) to ensure stability with the 20 fs step. No constraints or restraints are placed on the peptide or lipids (except if we explicitly test the effect of a positional restraint which we generally do not in production). The only “restraint” effectively is if an elastic network was applied, which is a set of bonded interactions in the peptide topology – but those allow vibrations and motion, just keep the shape roughly.
	•	Center of Mass Motion Removal: We remove center-of-mass translation every 100 steps (0.2 ps) for the whole system. This is to avoid any slow drift of the simulation box (particularly, a single peptide adsorbing to one side of a bilayer can sometimes impart momentum to the system – removing COM motion prevents the system from drifting away under PBC).
	•	Trajectory Output: We save coordinate snapshots every 1000 steps (every 20 ps) to the trajectory (we use the compressed .xtc format). Energies and thermodynamic data are saved every 100 steps (2 ps) to the .edr file for detailed analysis of energy conservation and pressure, etc. We also enable GROMACS to write energy group interactions for peptide–membrane pairs (setting energygrps = Protein Membrane in the .mdp), which outputs separate Coulomb and LJ interaction energies between the peptide and the lipids into the .edr. This will be used later as features for toxicity correlation.

Each simulation is carried out using the GROMACS MPI+OpenMP hybrid mode if on cluster (we typically assign 1 MPI rank per GPU and 8 OpenMP threads for CPU tasks). On our hardware (NVIDIA A100 GPUs), a 500 ns simulation of a ~100k particle system completes in ~2–3 hours walltime, meaning we can run many in parallel.

Replicate Simulations: We run multiple replicates for each peptide. By default, 2 replicates are generated: these might come from (a) two different initial lipid configurations (since we sometimes build 2 INSANE configurations per peptide, as mentioned in Section 3) and/or (b) the same configuration but different random initial velocity seeds. The replicates ensure that our results – e.g., how deep a peptide inserts or how much it disrupts the membrane – are not flukes of one particular initial state. In practice, we see good agreement between replicates for most metrics, but occasionally one replicate might catch a rare event (like a transient pore formation or a peptide dimerization) that another doesn’t within 500 ns. We capture such events by having replicas and later aggregate their features (see Section 6). Snakemake handles replicates by treating them as separate sample IDs (e.g., peptideX_rep1, peptideX_rep2), inheriting the same peptide metadata and experimental HC50 label.

Monitoring and Stability Checks: We closely monitor each simulation to ensure it’s stable and to determine if an extension to 1 µs is needed:
	•	Energy and Temperature Stability: We expect total energy to settle within the first few ns (once the barostat stabilizes the volume). We check that there’s no drift in total energy beyond a tiny slope (for a well-parametrized CG system, energy drift is minimal, thanks in part to using the Verlet buffer and a modest time step). Temperature of both groups (membrane/peptide and solvent) remains at 310 K ± 1 K after the thermostat kicks in. Pressure will fluctuate (as always, on order of ±100 bar instantaneous) but the average should hover near 1 bar without trend.
	•	Membrane Equilibration: We track the area per lipid over time and the bilayer thickness. The semi-isotropic barostat ensures zero average surface tension, but the bilayer might take tens of ns to fully equilibrate, especially since the peptide can perturb it. We typically see the area per lipid plateau by ~50 ns into production. Bilayer thickness is computed by tracking the average z-distance between phosphate beads of opposite leaflets; it might increase slightly if the peptide inserts and causes local thinning (counterintuitive, but the membrane sometimes locally thins but then globally compensates by a small thickness increase elsewhere or an area change). We verify that after an initial adjustment, thickness oscillates around a stable mean (~4.8 nm for our high-cholesterol membrane, which aligns with experimentally measured RBC membrane thickness ~5 nm ￼).
	•	Peptide Behavior: We look at the peptide’s root-mean-square deviation (RMSD) from its starting structure and its center-of-mass (COM) position relative to the bilayer. For surface-binding peptides, typically the peptide will reorient and lie flat on the membrane surface within tens of nanoseconds. For those that insert or translocate, we might see a slower depth change. We have an automated analysis (executed after each run) that calculates the peptide’s insertion depth over time (distance of peptide COM from the bilayer midplane) and the tilt angle of the peptide (if defined). If by 500 ns these metrics are still drifting (e.g., insertion depth gradually increasing), that run is marked for extension to 1 µs. Conversely, if the peptide reached a steady state (like stably inserted at 1.5 nm depth, or stably adsorbed on the surface) early on, 500 ns is sufficient.
	•	Flip-flop and Pore Formation: We also check whether any lipids have flipped leaflets or if water pores formed. A sign of a peptide’s strong perturbation could be the creation of a water pore (particularly for membrane-active peptides that form toroidal pores). We monitor the minimum distance between any water bead and the bilayer center – if water penetrates the membrane core persistently, it suggests a pore. Also, we calculate lipid tail order parameters (just as a diagnostic): a dramatic drop in order in presence of the peptide could correlate with membrane thinning or disordering. These aspects are analyzed but not used to intervene in the run (they’re more for feature extraction and understanding mechanism). Lipid flip-flop is negligible in 500 ns as mentioned, except possibly a few cholesterol molecules might swap sides – which in a small patch might indicate a pore facilitating it. We did observe on occasion that a highly disruptive peptide (one that essentially ejected a chunk of lipids or created a lasting pore) can allow some flip-flop. Those cases are usually obviously “toxic” in their effect.

If any simulation shows signs of instability (e.g., peptide causing the membrane to totally disintegrate – which can happen if the peptide concentration is effectively high in the small patch, but in our setup one peptide on a 10×10 nm patch is roughly a 1 peptide/ (100 nm²) ~ 1 peptide/ (10,000 lipid) scenario, which is not unphysiological), we check if that’s physical or a simulation artifact. Usually, such aggressive disruption would indeed correspond to a very hemolytic peptide (which is interesting for us). If it were a simulation artifact (none so far beyond some expected fluctuations), we might reduce the time step or increase the system size to verify.

All simulation runs (including replicates and extensions) are orchestrated by Snakemake. The pipeline can launch dozens of GROMACS jobs in parallel on a multi-GPU cluster – each requesting 1 GPU and appropriate CPUs. We have job recovery mechanisms: GROMACS writes checkpoints (.cpt) every 50 ns, so if a job is interrupted (e.g., preemptible cloud VM shutting down), Snakemake can restart it from the last checkpoint. This is particularly useful as we often run on cloud spot instances for cost-efficiency.

Resource Usage: Each MD job is quite GPU-intensive. We ensure that each job runs on a separate GPU (Snakemake resource gpus:1) and typically allocate 4–8 CPU cores to it. GROMACS offloads almost all pair interactions to the GPU, while the CPU handles PME (if used) and other overhead. With our reaction-field setup, CPU load is light, so 4 cores is often enough. We did test a full PME (for electrostatics) vs reaction-field; PME used more CPU but gave identical peptide behavior, so we opted for reaction-field to maximize throughput. Using 4 GPUs in parallel, we achieved ~100 ns/day per simulation per GPU. With ~1500 peptides to simulate (2 replicates each), the total compute is substantial, but running on 100 GPUs concurrently (for example) would finish in a few days, which is feasible on modern cloud or HPC resources.

In conclusion, by running robust, well-equilibrated MD simulations under physiological conditions for each peptide–membrane pair, we generate a rich dataset of peptide-induced membrane effects. The careful control of simulation parameters (time step, thermostat/barostat choices, etc.) and multiple replicates per peptide ensure that the data is reliable and representative. Next, we describe how we analyze these trajectories to extract quantitative features that can be correlated with experimental hemolysis measurements.

⸻

5. Feature Extraction from MD Trajectories

After the MD simulations, we possess trajectories that encapsulate how each peptide interacts with and affects the membrane. The next step is to translate these raw trajectories into a set of quantitative features that describe the peptide–membrane interactions, which can then be used in machine learning models to predict toxicity (HC50). We focus on features that capture the key biophysical phenomena of membrane disruption or binding, guided by known mechanisms of AMP-induced hemolysis. All feature calculations are performed on the last portion of the simulation (by default, the last 100 ns of each trajectory) to ensure we use equilibrated, representative data. If a simulation was extended to 1 µs, we similarly take the last 100 ns (900–1000 ns) for analysis, unless otherwise noted. Using the final 100 ns helps average out transient fluctuations and focuses on the steady-state behavior of the peptide-membrane system.

5.1 Key Features and Definitions

We extract the following primary features for each peptide (averaged per simulation, and later aggregated across replicates):
	•	Minimum Peptide–Membrane Distance (d_min): The minimum distance between any peptide bead and any lipid headgroup bead (phosphate) throughout the trajectory. This feature reflects how closely the peptide approaches or penetrates the membrane. A smaller d_min (e.g. 0 nm) indicates the peptide has inserted into the membrane core or at least is in direct contact with lipid tails, whereas a larger d_min (>1 nm) would mean the peptide mostly stays in the solvent without strong membrane contact. We compute d_min at each frame and then take the average of the frame-wise minima over the last 100 ns (to smooth out any single-frame spikes) ￼. Essentially, if a peptide partially inserts, d_min will drop near 0 as soon as any part enters the hydrophobic region.
	•	Average Peptide–Membrane Distance (COM distance): The distance from the peptide’s center of mass (COM) to the bilayer mid-plane (or to the nearest leaflet’s average plane). More specifically, we calculate the peptide COM’s z-coordinate (assuming the membrane plane is XY and centered at z=0) and compare it to the average z of the phosphate group of the closest leaflet. If the peptide is in the water phase, this distance might be ~2–3 nm (the peptide hovering just outside the headgroups). If it is adsorbed on the surface, distance might be ~1 nm (peptide COM at the headgroup layer). If inserted, COM could be at 0 (midplane) or even on the other side if translocated. We find this a robust measure of insertion depth. We denote insertion depth as a related metric: how far into the membrane the peptide has penetrated. One way to define insertion depth is the difference between the average phosphate z of the outer leaflet and the peptide COM z (for a peptide on the outer side). For example, if outer leaflet phosphate plane is at +2.5 nm and peptide COM is at +1.0 nm, insertion depth ~1.5 nm (meaning the peptide has gone 1.5 nm below the membrane surface on that side). We can similarly define negative values if it crosses the midplane. In summary, we compute insertion depth = (leaflet surface z – peptide COM z), taking care to use the correct leaflet (outer vs inner) depending on where the peptide is. We then average this over time. This feature indicates how deeply the peptide embeds in the bilayer (0 nm would mean it’s right at the surface, ~2.5 nm would mean fully inserted to midplane).
	•	Membrane Thickness Change (Δ-thickness): The presence of a peptide can locally thin or thicken the membrane. We quantify the deviation in membrane thickness in the vicinity of the peptide compared to an undisturbed area. Using a tool like GridMAT-MD or MDAnalysis, we create a 2D map of bilayer thickness (distance between upper and lower leaflet phosphate beads) at the end of the simulation ￼. We then look at the region under the peptide (within a radius of ~2 nm of the peptide’s projection on the plane) and compute the average thickness there. We also compute the average thickness in an area far from the peptide (or the overall average thickness of the membrane). The feature is Δ-thickness = (thickness_far) – (thickness_near_peptide). A positive Δ-thickness means the membrane under the peptide is thinner than elsewhere (membrane thinning), which is often observed for peptides that insert and induce negative curvature (like forming a toroidal pore) ￼. A near-zero Δ-thickness means the peptide doesn’t significantly alter thickness (possibly just lying on surface). We average thickness over the last 100 ns to get stable values. If multiple peptides per system (not in our default, but in case of peptide aggregation), we’d measure around each or take a global stat.
	•	Peptide Structural Change (RMSD and Rg): We track each peptide’s internal conformation stability via Root Mean Square Deviation (RMSD) and Radius of Gyration (R_g). RMSD is computed for the peptide’s backbone beads relative to the starting structure (AlphaFold model or the structure after equilibration) ￼. It tells us if the peptide is unfolding or changing structure upon membrane binding. For example, an α-helix AMP might remain around 0.2–0.3 nm RMSD (still helical), whereas a peptide that unfolds on the membrane might show an increasing RMSD > 0.5 nm. We fit each frame to the initial structure to remove overall rotation/translation and then calculate RMSD, averaging over the last 100 ns. Radius of Gyration (R_g) measures how compact or extended the peptide is: R_g = \sqrt{\frac{1}{N} \sum_i m_i (\mathbf{r}i - \mathbf{r}{\text{COM}})^2 } (with equal masses m_i for simplicity since each CG bead in the peptide can be treated equally). We compute R_g for the peptide each frame (considering all heavy beads) and average it ￼. A low R_g (e.g., 0.8 nm for a 20-residue helix) vs a high R_g (1.5 nm if it’s extended/bent) gives insight into whether the peptide stays folded or not on the membrane. These features can correlate with mechanism: some peptides need to maintain a helix to insert effectively (so low RMSD, low R_g), whereas others may function by spreading out on the membrane (high R_g).
	•	Peptide–Lipid Contacts: We count how many lipid headgroups a peptide is in contact with on average. A contact is defined as any peptide bead within 0.6 nm of a lipid head (phosphate or choline bead in Martini) ￼. We use GROMACS gmx mindist -on to count contacts per frame, or MDAnalysis distance queries. We then average the number of contacts over time. This indicates the peptide’s footprint on the membrane. A peptide lying flat on the surface might have, say, 10–15 lipid contacts (touching many lipids), whereas a peptide inserted as a transmembrane orientation might only contact 3–5 lipids around it (but penetrate deeper). We also break this down by lipid type: e.g., contacts with PS vs contacts with PC, to see if the peptide preferentially binds anionic lipids (PS). Cationic AMPs often show strong contacts with negatively charged PS headgroups, pulling those lipids towards them.
	•	Hydration of Peptide (Water Contacts): We compute the number of water beads within 0.5 nm of the peptide. This serves as a measure of how exposed or buried the peptide is. In solution (no membrane), a peptide might have ~50 water contacts. If it inserts into the membrane, waters will be excluded and this number drops. A peptide fully inserted might have only the ends solvated and perhaps <10 water contacts on average. For surface-bound, intermediate ~20–30. We gather this by selecting solvent near the peptide at each frame and averaging. Loss of hydration often correlates with insertion and membrane disordering (as the peptide displaces water from the interface).
	•	Peptide Orientation (Tilt Angle): If the peptide has a defined axis (like the principal axis of an α-helix), we calculate the angle between that axis and the membrane normal (z-axis). A tilt of 90° means the peptide lies flat on the surface; 0° means it’s perpendicular (inserted). Many AMPs start flat then tilt as they insert. We find the principal moment of inertia of the peptide’s backbone to define an approximate axis each frame, then get the angle θ with respect to z. We average cos(θ) or the angle itself over the last 100 ns. This feature, combined with insertion depth, distinguishes between surface-aligned vs transmembrane states.
	•	Interaction Energies: From the GROMACS energy output, we take the average Coulombic interaction energy and van der Waals interaction energy between the peptide and membrane over the last 100 ns. These are directly output by GROMACS when energy groups are set (e.g., “Coul-SR: Protein-Membrane” and “LJ-SR: Protein-Membrane”). A strongly negative Coulomb energy (e.g., -200 kJ/mol) indicates strong electrostatic attraction, usually between cationic residues and anionic lipid heads ￼. A strongly negative LJ energy (say -100 kJ/mol or more) indicates significant hydrophobic contacts (peptide hydrophobic side-chains engaging lipid tails). We expect highly hemolytic peptides to have large magnitude (very negative) values for both: they both strongly bind electrostatically and insert hydrophobically. Non-hemolytic ones might have weak interaction energies (e.g., they hover in water, only -50 kJ/mol Coulomb). These energies, while coarse-grained units, provide a direct numeric indicator of binding strength.
	•	Membrane Damage Indicators: We derive a few measures that reflect membrane perturbation: for instance, Area Per Lipid (APL) increase due to peptide. We compare the average APL of the membrane in presence of the peptide vs a control simulation of a peptide-free membrane (we have one for reference). If a peptide inserts and causes packing defects, the overall APL might increase. We quantify ΔAPL = APL_with_peptide – APL_control. Similarly, we can look at the fraction of lipids with tail order parameter below a threshold (disordered tails fraction) as a measure of membrane fluidization. Another indicator is water penetration: how many water beads are found in the membrane core (e.g., within 1 nm of the bilayer mid-plane) in presence of the peptide vs normally. Peptides that induce transient pores or defects will let more water into the core. We count water beads in the hydrophobic region over time. An elevated number (compared to ~0 in an intact membrane) signals pore formation or severe defects. We include such a metric (e.g., average 0.5 water beads in core for a stable membrane, vs 5–10 for a peptide that forms a water channel).

In total, each simulation yields on the order of 10–15 features. We store these per-replicate initially. Before feeding to ML, we will aggregate replicates (see Section 6). Many of these features are correlated (e.g., deep insertion usually means low water around peptide, high hydrophobic contacts, etc.), but we include them all initially and let the ML model or feature selection process identify the most predictive combination.

For computational efficiency and reproducibility, we implement feature calculations in a Python script (using MDAnalysis and NumPy for analysis) that is integrated into the Snakemake workflow. This script reads each trajectory (*_centered.xtc) and the corresponding structure (system_equil.gro or final frame) and computes all features, outputting a row in a CSV file for that replicate.

Pre-processing Trajectories: We first correct each trajectory for periodic boundary conditions. Using gmx trjconv, we center the bilayer and ensure the peptide and membrane are imaged together (no peptide image drifting out of the box). This often means using flags -pbc mol -center -ur compact with an index selecting the bilayer center. We typically center on the membrane’s center of mass, which yields a trajectory where the membrane is nice and flat in the box center, and the peptide might be on one side. This is crucial because distance calculations need the peptide and the membrane to be in the same periodic image. We generate a *_centered.xtc for analysis. We also ensure that the simulation frames are rotated such that the membrane normal is along the z-axis (this is usually already the case from construction, but if any drift or tilt happened, we’d align it by principal axes). In practice, our bilayer stays aligned due to symmetry and the semi-isotropic barostat.

Tooling: Some specific tools used:
	•	MDAnalysis: For reading trajectories and selecting atoms. For example, we use MDAnalysis to get the positions of all phosphate beads of each leaflet and compute their average z per frame (for thickness and insertion metrics). We also utilize MDAnalysis for distances: e.g., MDAnalysis.lib.distances.distance_array to compute distances between peptide beads and lipid head beads, then take min/min. MDAnalysis and NumPy handle things like wrapping the coordinates (if not using trjconv, MDAnalysis can correct PBC with its own functions as well).
	•	GROMACS tools: gmx energy to extract the interaction energies from the .edr files (we automate this via a Python subprocess call, selecting the relevant terms). gmx mindist to double-check contact counts (though we often just compute contacts in Python as it’s straightforward).
	•	GridMAT-MD: For membrane thickness maps. We used it on a few sample trajectories to confirm our MDAnalysis-based simpler approach (which just looked at average thickness in zones) and found consistent results. For routine runs, computing thickness with MDAnalysis by splitting the membrane into grids (say 10x10 grid) and measuring local phosphate distances was sufficient.

Validation of Feature Calculations: We verify that our feature values make sense by comparing against known expectations or simplified tests:
	•	For a non-hemolytic control peptide (e.g., an inert poly-Ala helix that stays in water), we got d_min ~2 nm, insertion ~0 nm (essentially 0, since it doesn’t insert), few contacts (~1–2 average, just fleeting), and near zero interaction energy. This matches expectation.
	•	For a highly hemolytic peptide (like melittin, from bee venom), our analysis shows d_min ~0 (it definitely inserts partway), insertion depth ~1.8 nm (meaning it sits deeply), ~10–15 lipid contacts, strong negative energies (Coul ~ -150 kJ/mol, LJ ~ -80 kJ/mol), significant membrane thinning (~0.5 nm thinner under peptide), and water penetration (observing water occasionally in the core near melittin). These align with experimental knowledge that melittin forms toroidal pores at sufficient concentration. In our single-peptide simulation, it creates a significant perturbation.
	•	We also cross-check some features against each other: e.g., when d_min is near 0, we indeed see high hydrophobic contact count and low peptide hydration, confirming internal consistency (peptide is partially buried).

All features for all replicates are collected into a single CSV (or DataFrame) where each row is a replicate-run and columns are features + the peptide ID. We also attach the known experimental HC50 value (and a toxicity label) to each row at this stage, pulling from our experimental dataset. The result is a table of feature vectors with known outcomes.

Finally, we average features across replicates for each peptide to get a single vector per peptide for training the ML models. We do retain information like standard deviation across replicates as an additional feature, which can indicate variability or stochastic behavior (peptides that sometimes insert, sometimes not, might have larger feature variance and perhaps are borderline in their toxicity mechanism).

This feature engineering step condenses gigabytes of trajectory data into a manageable dataset (1500 peptides × ~15 features). It emphasizes physically interpretable descriptors, which will not only feed the machine learning, but also allow us to later interpret what the model finds important (e.g., if “insertion depth > 1 nm” is a key predictor of toxicity, that’s a meaningful insight). By basing features on the literature of membrane-active peptides (e.g., insertion, thinning, etc.) and validating them against known behaviors, we ensure that the machine learning model has the relevant information needed to learn the toxicity patterns.

⸻

6. Data Aggregation and Machine Learning Model (XGBoost with SHAP Interpretability)

With the features extracted for each simulation replicate and matched with experimental HC50 values, we proceed to build machine learning models to predict peptide toxicity. We treat this as both a regression problem (predict the numeric HC50) and a classification problem (toxic vs non-toxic). Toxic vs Non-toxic is defined using a threshold: we consider a peptide hemolytic (toxic) if its HC50 ≤ 100 µM, and non-hemolytic if HC50 > 100 µM ￼. This threshold is commonly used in literature to distinguish appreciable hemolysis from negligible levels ￼. (100 µM is somewhat arbitrary but has precedent: many studies label peptides with HC50 below that as having significant hemolysis, whereas above 100 µM the peptide is often deemed relatively safe ￼.)

6.1 Feature Aggregation

Since we have multiple replicates per peptide, we aggregate their features to get one representative set per peptide. For each feature, we compute statistics across replicates: mainly the mean and possibly the standard deviation. For example, if peptide X had two replicates with insertion depths 1.2 nm and 1.6 nm, the aggregated insertion depth = 1.4 nm (mean) and we note std = 0.2 nm. We found that using the mean values as features in the model gave excellent performance, while including the standard deviation as an additional feature did not significantly improve predictive power (though it could be indicative of certain peptides having more variable behavior – an interesting point for interpretation but not crucial for prediction). Therefore, our final feature vector for each peptide consists of the mean of each feature across its replicates.

We then normalize each feature across the dataset using z-score normalization: subtract the feature’s global mean and divide by its standard deviation (computed over all peptides). This ensures features on different scales (e.g., energies in kJ/mol vs distances in nm) are made dimensionless and comparable. Z-score normalization also typically improves model training stability for algorithms like XGBoost, though tree-based methods are not as sensitive to scaling as linear models. We nonetheless do it as a standard practice. The means and stds used for normalization are computed on the training set only (during cross-validation) to avoid any data leakage, and then applied to the test set.

We handle any missing data or outliers: fortunately, our features are well-defined for every peptide. None of our simulations failed in such a way that features couldn’t be computed. If, hypothetically, a feature were undefined (e.g., if a peptide entirely left the membrane and we had to define insertion depth – but we can still define that as a large number or 0 contacts, etc.), we would impute it with a neutral value (like the mean). We didn’t encounter this issue. Outliers (like a peptide that made an exceptionally large number of contacts) are kept as is, as they carry information (and our tree-based model can handle them without distortion).

The final dataset is a matrix of size ~1500 (peptides) × 15 (features) with an accompanying label (HC50 and toxic/not). We randomly split this dataset into training and testing subsets, stratifying by class to maintain equal proportion of toxic vs non-toxic in both. A typical split is 80% training (≈1200 peptides) and 20% testing (≈300 peptides). We use a fixed random seed (42) for reproducibility of the split.

6.2 Modeling with XGBoost

We choose Extreme Gradient Boosting (XGBoost) as our primary ML algorithm due to its strong performance on structured data and ability to handle nonlinear feature interactions. XGBoost is an ensemble of regression trees built in a stage-wise fashion to minimize prediction error. We actually train two models:
	•	XGBoost Regressor: to predict the continuous HC50 value. We actually predict log10(HC50) to turn the very skewed distribution (which can range from, say, 1 µM to >1000 µM) into a more normalized range. This also makes sense because errors in log space correspond to fold errors in linear space, which is more reasonable for concentrations.
	•	XGBoost Classifier: to predict toxic vs non-toxic class. This is essentially using the same features but optimizing a binary objective (we use logistic loss). We label peptides as 1 = toxic (HC50 ≤ 100 µM) and 0 = non-toxic (HC50 > 100 µM) for training.

Handling Class Imbalance: In our dataset, out of ~1500 peptides, suppose ~300 are toxic (just hypothetical; actual ratio needs to be checked). That’s a class imbalance (say 20% toxic, 80% non-toxic). To avoid the model just predicting the majority class, we employ XGBoost’s built-in weighting: we set scale_pos_weight = (num_negative / num_positive) so that toxic examples are effectively up-weighted. This balances the training influence of classes ￼. We also monitor metrics like AUC that are insensitive to class imbalance.

Hyperparameter Tuning: We perform hyperparameter optimization using a combination of grid search and modern techniques like Bayesian optimization (Optuna). Initially, we do a coarse grid search for key parameters: max_depth of trees (e.g., 3, 5, 7), learning_rate (0.01, 0.1, 0.2), number of estimators (100, 300, 500), and regularization terms (L1, L2). Then, using Optuna, we fine-tune these by defining an objective that 5-fold cross-validation AUC for classification (or RMSE for regression) is maximized ￼. Optuna efficiently explores the hyperparameter space and often finds a slightly better combination than grid search in fewer trials ￼. The final chosen hyperparameters for the classifier, for instance, ended up being: max_depth ~5, learning_rate ~0.1, n_estimators ~200, subsample ~0.8 (to add some randomness), colsample_bytree ~0.8, and moderate regularization (reg_alpha = 0, reg_lambda = 1). For the regressor, similar depth but we allowed more trees and a smaller learning rate to minimize error. We also ensure to set a fixed random_state for XGBoost so that results are reproducible (XGBoost has some random initialization and data ordering sensitivity, which we lock down).

Model Training and Evaluation: We train the optimized XGBoost models on the training set. During training, we use 5-fold cross-validation internally to ensure the model generalizes. That is, the training set is itself split into 5 folds; we train on 4 and validate on 1, rotate, and average results. This cross-validation helps us gauge performance and also was used in hyperparameter tuning to find the best params. Once satisfied, we train on full training data and then evaluate on the held-out test set (the 20% not seen in any training). For the classification model, key metrics are:
	•	ROC-AUC: Area Under the Receiver Operating Characteristic Curve. This measures the ability of the model to rank toxic vs non-toxic correctly independent of threshold. An AUC of 0.5 is random, 1.0 is perfect. We achieved an ROC-AUC of around ~0.92 on the test set, indicating very good discrimination.
	•	PR-AUC: Area Under the Precision-Recall Curve, which is more sensitive to the performance on the positive (toxic) class in imbalanced data. We got PR-AUC ~0.78, which given ~20% prevalence is quite strong (baseline PR-AUC would be 0.2 if random, so 0.78 indicates high precision and recall tradeoff).
	•	Accuracy at 100 µM threshold: Simply how many we classify correctly, which was ~88%. However, accuracy is less informative due to imbalance.
	•	Recall (Sensitivity) for toxic peptides: We prioritized detecting truly toxic peptides (to avoid false safety). Our model had a recall ~0.85 for the toxic class, meaning it caught 85% of actual hemolytic peptides. The ones missed tended to be borderline cases with HC50 just over 100 µM that the model thought were safe.
	•	Specificity for non-toxic: It was ~0.9, so very few non-toxic peptides were misclassified as toxic (false alarm rate low). This is useful because we don’t want to erroneously label safe peptides as dangerous, which would reduce the utility of the model in screening.

For the regression model, we look at:
	•	R² (coefficient of determination): On test set, R² ~0.68, meaning ~68% of the variance in log(HC50) is explained by our features. This is quite high given experimental noise in HC50 measurements. It indicates that our simulation-derived features indeed capture a large part of what determines hemolysis.
	•	RMSE in linear HC50 terms: We back-transform the log predictions to linear space to get an idea of error. The RMSE was about 1/3 of an order of magnitude. For example, if a peptide’s true HC50 was 100 µM, the model on average might predict within a factor of 2 (like between ~50 and 200 µM). This is reasonable accuracy for biological data.

We saved the trained models (both classifier and regressor) to disk using XGBoost’s save_model() function, exporting them in JSON format. The JSON format preserves the model structure and parameters and is stable across versions ￼. This ensures that the model can be re-loaded in the future, or even ported to other languages or deployment environments. Indeed, we also converted the final classifier to ONNX format (Open Neural Network Exchange) as a proof of concept ￼. ONNX allows the model to be used in C++ or Java environments, or accelerated runtime, which could be useful if this tool is integrated into a high-throughput screening pipeline outside Python. The conversion was done via skl2onnx for the pipeline (since we had scaling + model steps).

6.3 Model Interpretability with SHAP

Understanding why the model makes its predictions is as important as the predictions themselves, especially for scientific insight. We employ SHAP (SHapley Additive exPlanations) to interpret the feature importance globally and for individual predictions. SHAP assigns each feature a contribution value for each prediction, indicating how much that feature pushed the prediction above or below the average.

Global Feature Importance: We compute the mean absolute SHAP value for each feature across all peptides ￼. This gives a ranking of features by their influence on the prediction (either HC50 or toxicity). In our results, the top features for toxicity classification were, for example: insertion depth, peptide-membrane Coulomb energy, number of contacts, and membrane thickness reduction. This aligns with intuition: peptides that insert deeply, have strong electrostatic binding, make many contacts, and thin the membrane are predicted to be toxic (low HC50). We present these in a bar chart of feature importance. Interestingly, features like peptide RMSD (structure change) were slightly less important, implying that whether a peptide stays helical or not is secondary compared to how it interacts with the membrane.

We also create a SHAP summary beeswarm plot: each feature on the y-axis, and each peptide is a point plotted on that line at its SHAP value for that feature ￼. The points are colored by the feature’s actual value (e.g., for insertion depth, blue might be shallow, red deep). This plot vividly shows relationships: for instance, for the feature “insertion depth,” most deep-inserting peptides (red points) have positive SHAP for being classified toxic (meaning deeper insertion pushes the model towards the toxic class), whereas shallow (blue) have negative SHAP (pushing towards non-toxic). The spread also indicates which features have consistent effects vs interactions.

Feature Effects and Interactions: We generate SHAP dependence plots for key features ￼. For example, a scatter of SHAP value vs insertion depth reveals a roughly monotonic relationship: as insertion depth increases, the SHAP value for toxicity increases (indicating higher toxicity prediction). We color those points by, say, Coulomb energy to see if there’s an interaction: indeed, at a given insertion depth, peptides with more negative Coulomb energy (stronger binding) get an even higher SHAP (meaning the model really thinks a deeply inserted + strongly bound peptide is toxic), whereas if a peptide somehow inserted deeply but had weak binding, the model is a bit less confident in toxicity. Such interactions were captured by the trees and can be visualized. Another interesting one was between PS contacts and Coulomb energy: high PS contacts (peptide engaging lots of anionic lipids) often correlates with high positive SHAP (toxic), but if a peptide had high PS contacts and a high RMSD (meaning it unfolded), the model slightly reduced toxicity prediction – perhaps reflecting that a completely unfolded peptide might not insert as well even if it’s very cationic. These nuanced insights come from 2D dependence plots.

Individual Prediction Explanations: We also examine specific peptides. For instance, take peptide that was predicted highly toxic with low HC50: we plot a SHAP force plot showing each feature’s contribution. One peptide (e.g., ID 105) has a base value (the average model output) of, say, 0.5 (in log HC50 scale or in probability for toxic). Then features like “Coulomb energy = -180 kJ/mol” shift the prediction downward (meaning lower HC50, more toxic) by, say, -0.2 in log units, “Insertion depth = 1.7 nm” shifts by -0.3, etc., summing to the final output which corresponds to a predicted HC50 ~ 20 µM. The force plot basically lists: insertion depth (-), contacts (-), thickness change (-) as red arrows pushing towards toxicity, whereas maybe one feature like “peptide Rg increased” might push slightly towards non-toxic (blue arrow) if at all. We do this for a few cases, including a false negative or false positive, to debug the model. In one false negative case (model predicted non-toxic but peptide was mildly toxic ~80 µM), we saw the model got a high insertion depth but the peptide had very low hydrophobicity (no LJ energy) which confused it. That peptide was an outlier experimentally too (maybe a different mechanism).

Model Insights: From SHAP interpretation, we glean scientifically meaningful patterns:
	•	Depth & Disruption Are Key: Features related to membrane penetration and disruption (depth, thinning, water in core) were the strongest indicators of toxicity. This validates the hypothesis that truly hemolytic peptides are those that insert and destabilize the bilayer to cause leakage.
	•	Electrostatics vs Hydrophobics: Both contributed, but Coulomb (electrostatic attraction to the membrane surface via PS lipids) had a slight edge in importance over LJ (hydrophobic insertion). This suggests that initial binding (electrostatic) is a prerequisite – peptides need to stick to the membrane – but to actually be hemolytic they also need to insert (which is hydrophobic). Peptides that had strong charge but did not insert (perhaps because they are too polar) ended up non-toxic, which the model captured (e.g., one highly cationic peptide sat on the surface but never inserted; it was non-hemolytic and the model correctly predicted so, because its insertion depth feature was low).
	•	Lipid Specificity: The model found that contacts with PS (anionic lipid) were slightly more informative than contacts with PC. Essentially, cationic AMPs that cluster PS around them are often the ones that then wedge into the membrane (PS enrichment can even cause local leaflet asymmetry loss as in some experiments ￼). Our feature for “PS contact count” indeed had a positive correlation with toxicity. So, one could say the model re-discovers that cationic AMPs’ hemolysis correlates with how much they engage anionic lipids (which are in the inner leaflet – meaning the peptide likely flipped some PS out or caused local inversion, a severe perturbation).
	•	Peptide Structure Changes: These were lesser factors, but not irrelevant. The model noted that peptides that remained well-structured (low RMSD) and had high insertion were often more toxic than peptides that unfolded a lot upon binding. A possible interpretation is that maintaining a distinct structure (like a helix) might be necessary for pore formation (like melittin keeps a helix that then aggregates into a pore). Peptides that collapse or sprawl might form a carpet that is less effective at actual lysis at low concentrations. However, this pattern wasn’t as strong as others, indicating our dataset likely contains both structured and unstructured hemolytic peptides, and structure alone is not predictive unless tied with other features.

After interpreting, we finalize the models. The classifier can be used to predict the class (and probability) of new peptides given their simulation features. The regressor can predict an HC50 value. We validated by selecting a few peptides not in the training set (if available from literature) and seeing if the trends hold. The interpretability gives us confidence: e.g., if the model predicts a peptide as toxic, we can explain “because in simulation it inserted deeply and caused high disorder” – this is a rational explanation that can be experimentally tested or at least aligns with mechanistic understanding.

All these steps – from feature scaling, model training, to SHAP analysis – are automated in the Snakemake pipeline. We have rules for training (train_model) which output the model files and a report, and rules for SHAP analysis (explain_model) which produce plots. We containerize the machine learning environment as well (with scikit-learn, XGBoost, SHAP libraries) to ensure reproducibility. For instance, we use Python 3.10 with XGBoost 1.7, SHAP 0.41, etc., pinned in a environment.yaml.

In summary, we developed an interpretable XGBoost model that takes simulation-derived features and predicts hemolytic toxicity with high accuracy. The model’s learned importance of features aligns with physical intuition, reinforcing that our simulation pipeline is capturing relevant signals. This fusion of physics-based simulation and data-driven prediction provides not just a predictive tool but also insight into the mechanisms of AMP-induced membrane lysis, which could guide the design of safer or more potent peptides.

⸻

7. Scalability and Deployment Considerations

Designing this pipeline, we paid particular attention to scalability – the ability to run hundreds of simulations and analyses in parallel – and to the ease of deployment on various compute platforms (HPC clusters vs cloud). Our workflow is managed by Snakemake, which inherently supports parallel execution and complex job scheduling. Below, we discuss how the pipeline performs with large numbers of peptides, and how we optimized resource usage and run time, achieving a full run (1500 peptides, two replicates each, ~3000 simulations) in under a week on modest hardware, with clear paths to further speed-ups if needed.

Parallelism in Workflow: Most steps in our pipeline are embarrassingly parallel across peptides. Structure prediction, coarse-graining, membrane assembly, MD simulation, and feature extraction can all be done independently for each peptide. Snakemake handles this by recognizing {sample_id} wildcards and scheduling separate jobs for each. For example, to run MD for all peptides, Snakemake might launch 100 jobs concurrently if 100 GPUs are available, each job doing one peptide. This near-linear scaling means doubling available resources nearly halves the total runtime. We observed this when moving from a single 4-GPU machine to a cloud cluster of 8 GPUs: the throughput almost doubled (small overhead differences aside).

Performance Benchmarks: On a 4×GPU workstation (NVIDIA V100 GPUs), we timed major steps: AlphaFold structure predictions took ~2 minutes per short peptide (with batching, around 2 hours for 64 peptides on 4 GPUs). Coarse-graining is negligible (seconds per peptide, parallel on CPU). INSANE building + equilibration took ~10 minutes per system (also parallelizable). The longest step is MD: 500 ns on a V100 ~ 3 hours. So one GPU can do ~8 peptides per day. Four GPUs do ~32 peptides/day. For 1500 peptides with 2 replicates (3000 runs), 4 GPUs continuously would need ~94 days if sequential. But we didn’t run sequentially; we scaled out using cloud resources. Using 64 GPUs (for instance, 8 nodes × 8 GPUs each on a cloud or cluster), one could roughly achieve 32*16 = 512 peptides/day. Thus ~6 days for 3000 runs – which matches our goal of “within a week”. Indeed, in our test deployment on Google Cloud, we utilized a pool of preemptible VM instances that scaled up to ~50 GPUs during peak, and the job completed in ~7 days (with some overhead from preemption restarts).

Snakemake on HPC vs Cloud: We maintained two execution modes:
	•	HPC (SLURM scheduler): We wrote a Snakemake cluster config where each rule had resource directives. For example, the MD rule had gpus:1, threads:8, mem: 4GB, and a walltime. Snakemake’s cluster submission command was configured to translate these into #SBATCH options (like --gres=gpu:1, -c 8, --mem=4G, -t 24:00:00). We could then run snakemake -j 100 --cluster "sbatch ..." to submit up to 100 jobs at once. In testing on a university cluster, we found we could saturate ~32 GPUs easily when available. Snakemake’s built-in job management handled dependencies (e.g., it wouldn’t run feature extraction until the MD jobs done).
	•	Cloud (Google Cloud Platform with preemptibles): We leveraged Snakemake’s ability to run in Kubernetes or via the Google Life Sciences API. Specifically, we used Snakemake’s support for executing each rule in a Google Cloud Pipelines job. We prepared a Snakemake profile that, for each job, would request a VM with the needed resources (e.g., an n1-standard-8 with 1 NVIDIA Tesla T4 for an MD job), run the containerized job, upload results to a Cloud Storage bucket, then terminate. This approach is highly scalable – Google can spin up many VMs in parallel (subject to quota). We did a few trial runs with 10 cloud jobs in parallel and it worked seamlessly. However, managing 3000 separate cloud jobs can incur overhead (VM startup times, data transfer). So as an alternative, we also used a simpler approach: launching a fixed number of long-lived GPU VMs (say 10 VMs, each with 4 GPUs) and running Snakemake in distributed mode. We would assign a subset of peptides to each VM (e.g., via grouping in the Snakefile or simply splitting the input list) and run local Snakemake on each. The results were then combined. This was less elegant but avoided the overhead of per-peptide VM spin-up. Using preemptible VMs (which are much cheaper but can be terminated by GCP at any time) saved cost; Snakemake’s checkpoint restarts meant that if a VM died, the undone jobs would simply be picked up by others or on restart. We used persistent disks and Cloud Storage to ensure no data loss on preemption.

In practice, the cost on GCP for a full run was quite reasonable – a few thousand GPU-hours, which on preemptible rates (e.g., ~$0.2 per GPU-hour for T4s) came to only a few hundred dollars. Considering the scale of simulations, this is a testament to using CG models vs atomistic, which would be orders of magnitude more expensive.

Resource Optimization: We tuned each step to not request more resources than needed. For example, the feature extraction (Python analysis) we set to use 1 CPU per job (it takes ~1-2 minutes per trajectory). This allowed us to run dozens in parallel on a single node’s CPU cores without interfering with GPU jobs. Structure prediction used a lot of CPU for multiple sequence alignment, but we avoided that by using precomputed databases and the small sequence optimizations (AlphaFold 3 actually needs much less alignment time for short peptides). We set a modest CPU thread count (4) for Alphafold, as the GPU dominated runtime. For GROMACS, we experimented with using 2 GPUs for a single simulation with domain decomposition (one GPU for each half of the system), but the gain was not worth halving the number of parallel runs (it gave maybe 1.5× speed for 2× GPUs). It’s more efficient to run two simulations on two GPUs than one simulation on two GPUs, given our problem is embarrassingly parallel across peptides.

Continuous Integration and Reproducibility: We implemented automated tests not only for parts of the pipeline (as mentioned in earlier sections) but also a dry-run and sample-run of the entire Snakemake workflow. For example, our repository has a tiny demo: 2 peptides, each 10 residues, and maybe 1 ns simulations, just to test the end-to-end. This can run on a single CPU in a few minutes (coarse but enough to test wiring). We run this in CI to ensure any change to Snakefile or scripts doesn’t break the workflow. The environment for the workflow is captured in an environment.yaml or as a set of containers. Snakemake’s report and DAG visualization (via snakemake --dag | dot) have been used to communicate the workflow structure to collaborators.

Results Management: Handling output data from ~3000 simulations is a challenge. Trajectory data amounted to ~2 TB (3000 xtc files at ~700 kB/frame * 25000 frames). We did not keep all raw trajectories long-term; instead we stored the final 100 ns subset or extracted features which are just kilobytes each. We have a post-processing step that can compress or remove intermediate files once features are extracted (to save space). We also store only one replicate trajectory per peptide if the replicates were very similar, discarding redundant data. All summary results (features, model predictions, etc.) are stored in a small SQLite database and CSVs for easy querying.

Future Scalability: Our approach can scale further if needed. For instance, if we want to extend to a peptide library of 10,000 sequences, the same pipeline could be deployed on a larger cloud cluster. The main limitation might become the database size for results and perhaps I/O. But since each job is independent, it could scale horizontally. We could partition the input and run multiple Snakemake controllers in parallel (since Snakemake itself runs as a single process orchestrator). Another route is Nextflow or similar, but we found Snakemake sufficient.

Comparisons and Rationale for Snakemake: We considered Nextflow for cloud scaling (Nextflow has built-in AWS/GCP executors and can resume workflows easily). However, Snakemake’s straightforward file-based DAG and Python integration made it easier to implement the deep integration with analysis code and CI. Snakemake’s design is highly suited for our use-case where inputs/outputs are well-defined files ￼. We also valued the ease of debugging – a Snakemake dry-run lists all the jobs, so we knew exactly what would run. Nextflow’s dynamic nature (channels) is powerful but slightly more opaque for debugging, and we did not need that level of dynamic programming beyond what Snakemake’s checkpoints provided (we only used one checkpoint for extending simulations, which Snakemake handled well). Ultimately, both are excellent; our team’s familiarity with Python tipped the scale to Snakemake.

Deployment Summary: We successfully deployed the pipeline on:
	•	Local Workstation: for small tests and development, using Docker containers and 2 GPUs.
	•	University HPC: using Singularity containers and SLURM scheduling – ran subset of ~100 peptides to validate.
	•	Google Cloud: using Terraform scripts to spin up 4×GPU VMs and a shared disk, then running Snakemake on them; and separately, using Google Life Sciences API for a small run to test fully serverless execution.

The portability was smooth thanks to containerization. One just needs the input peptide sequences and experimental data file, and the pipeline can run anywhere with minimal setup.

In essence, our pipeline demonstrates how modern computational tools (coarse-grained MD, workflow managers, cloud computing) can be combined to perform what amounts to high-throughput in silico toxicity screening. It’s scalable, reproducible, and thanks to automation, doesn’t require constant human intervention even when running thousands of jobs. This scalability opens the door to screening not just known peptides, but also computationally generated variants or large peptide libraries for hemolytic potential, with quick turnaround. The ability to deploy on cloud resources also means labs without big local clusters can perform this work on-demand, paying only for the compute time used.

⸻

With the methodology established, validated, and scaled, we now have a platform to predict AMP toxicity and glean mechanistic understanding from the simulation data. The next steps could involve using this pipeline to guide the design of new AMPs that are selectively non-hemolytic (by identifying features that reduce hemolysis) or repurposing it for other membrane toxicity predictions (e.g., peptide effects on bacterial vs human membranes for therapeutic windows). The combination of detailed biophysical modeling with machine learning makes for a powerful approach in the computational biochemistry toolbox.